# 🧠 Build a Large Language Model (From Scratch), Sebastian Raschka

Welcome to my personal repository where I explore and implement the concepts from the book **[Build a Large Language Model (From Scratch)](https://www.manning.com/books/build-a-large-language-model-from-scratch)** by *Sebastian Raschka*. This project documents my journey through the book chapter by chapter — implementing attention mechanisms, training GPT-like models, and understanding large language models from the ground up.

---

## 📘 About the Book

This book is a hands-on, from-scratch guide to implementing GPT-style large language models using PyTorch. It covers:

- Data preprocessing and tokenization
- Self-attention and transformer architecture
- Building and training a GPT model from scratch
- Pretraining on unlabeled data
- Fine-tuning for classification and instruction-following tasks

---

## 🛠️ Tools & Frameworks

- Python 3.x
- PyTorch
- NumPy / Matplotlib
- Jupyter Notebooks
- tqdm / regex / other utilities

---

## 🚧 Work in Progress

This is a learning project. Expect some rough edges, refactoring, and iteration as I move through the chapters and experiments.

---

## 🎯 Goals

- Understand and implement the core components of a large language model.
- Get hands-on experience with pretraining and fine-tuning workflows.
- Experiment with training strategies on consumer hardware.
- Share code and insights to help others learning LLMs from scratch.

---

## 📚 Source Materials

- 📖 *Build a Large Language Model (From Scratch)* by Sebastian Raschka
- 🔗 [Book GitHub repo](https://github.com/rasbt/LLMs-from-scratch)
- 🧠 [Author’s site](https://sebastianraschka.com)

---

## 🤝 Contributions & Feedback

This is a personal study repo, but feel free to open issues or suggest improvements. Collaboration always helps learning!

---

## 📝 License

This repo contains original code based on educational exercises. All rights to the book content belong to the author and Manning Publications.

---
